---
title: "Bounds with Mislabeled Quartiles"
author: "Abby Steckel"
date: "10/26/2021"
output:
  word_document: default
  html_document: default
---

*The purpose of this file is to investigate what happens to the bounds when every sentence above 0.03 is categorized as "above 0.03", as opposed to categorizing sentences as "above first quartile", "above median", or "above third quartile," depending on the highest threshold they cross. Except for lines 83-88 and 103-111, this file contains the same code as "sentencing bounds.Rmd."*

*Conclusion: For the purpose of computing bounds, it does not matter how we classify df$sentences.quartiles, because the response variable in our treatment and control models is df$SENTENCE > a given sentence quantile‚Äù, and we create separate models for each quantile. Correct classification of df$sentence.quartiles may matter when we are trying to estimate how many people are affected by discrimination in each sentence level.*

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Read data 
```{r, message=FALSE}
library(R.utils)
library(tidyverse)
combine <- read.csv('/Users/abbysteckel/Desktop/combined.csv.gz')
combine <- na_if(combine, "")
#subset to just Black or white folks 
df <- combine[combine$RACE %in% c("black", "white"), ]
df$RACE <- ifelse(df$RACE == "black", 1, 0)
#dim(df) 554572 x 34
```


### KLM Bounds
$Y_i$ = sentence length 
$D_i$ = whether the case involves a Black or white person, where $D_i = 1$ if the person is Black and 0 if they're white
$M_i(d)$ = mediating variable. whether a person would have been included in the sample (convicted of at least as severe a crime) if they were of race $d$. For each observation in the dataset, $M_i(D_i)$ = 1. 

Naive estimator $\Delta = E[Y_i|D_i = 1, M_i = 1] - E[Y_i|D_i = 0, M_i = 1]$
KLM observe that the naive estimator is the weighted combination of average outcomes in the first three of the four principal strata: (1) always-convict (situations where both Black and white people would have been convicted), (2) anti-minority convictions (situations where the person wouldn't have been convicted, or would have received a less severe conviction, if they were white), (3) anti-white convictions, and (4) never-convict. 
\[
\begin{align*}
\Delta = E[Y_i|D_i = 1, M_i(1) = 1, M_i(0) = 1] P(M_i(0) = 1 | M_i(1) = 1, D_i = 1) + \\
E[Y_i|D_i = 1, M_i(1) = 1, M_i(0) = 0] P(M_i(0) = 0 | M_i(1) = 1, D_i = 1) - \\
E[Y_i|D_i = 0, M_i(0) = 0, M_i(1) = 1] P(M_i(1) = 1 | M_i(0) = 0, D_i = 0) - \\
E[Y_i|D_i = 0, M_i(0) = 0, M_i(1) = 0] P(M_i(1) = 0 | M_i(0) = 0, D_i = 0)
\end{align*}
\]

QUESTION: How is $ATE_{M=1}$ different from the naive ATE? 
```{r}
#on average and without considering selection bias, Black people are sent to prison for 42.41 months longer
mean(df$SENTENCE[df$RACE==1], na.rm = TRUE) - mean(df$SENTENCE[df$RACE==0], na.rm = TRUE)
```

KLM derive "large-sample nonparametric sharp bounds on the $ATE_{M=1}$ and the $ATT_{M=1}$." (10) 
$ATE_{M=1} = E[Y_i(1, M_i(1)) | M_i = 1] - E[Y_i(0, M_i(0)) | M_i = 1]$. This is the ATE of being Black on sentence length, among people who were convicted.

$ATT_{M=1}$ is the number of months that were added to the sentences of Black people who were convicted, which wouldn't have been added if they were white (regardless of whether they would have been convicted if they were white). 
$ATT_{M=1} = E[Y_i(1, M_i(1)) | D_i = 1, M_i = 1] - E[Y_i(0, M_i(0)) | D_i = 1, M_i = 1]$

### Adapting KLM's bounds_20190120_blackwhite.R
```{r, message=FALSE}
## data manipulation
library(reshape2)
library(plyr)
library(stringr)

## math
library(Matrix)
library(MASS)
library(speedglm)
library(sandwich)

## graphics
library(ggplot2)
#library(Cairo) throwing an error

## parallel
library(foreach)
library(doMC)
registerDoMC(cores = 8)

red <- '#A31F34'
blue <- '#325585'

#create sentence levels  
sentence.quartiles <- c(
  'above 0.03',
  'above first quartile',
  'above median', 
  'above third quartile'
)

## set white as base treatment level
races.abbr <- c('white', 'black')
races <- c('white', 'black')
df$race <- factor(df$RACE, labels = races)

outcome <- c('SENTENCE') 
predictors <- c('AGE', 'MALE', 'HSGED', 'SOMEPOSTHS', 'POSTHSDEGREE', 'HISPANIC', 'USCITIZEN', 'CRIMINAL', 'CATEGORY2', 'CATEGORY3', 'CATEGORY4', 'CATEGORY5', 'CATEGORY6', 'NOCOUNTS', 'POINTS', 'TRIAL','YR2001', 'YR2002', 'YR2003', 'YR2004', 'YR2005', 'YR2006', 'YR2007', 'YR2008', 'PRIM_OFFENSE', 'race') #26 predictors 

df <- na.omit(df[, c(outcome, predictors)])

quantile(df$SENTENCE) #first quartile: 15. median: 37. third quartile: 72 

#add a categorical sentence quartiles variable to the dataframe 
df <- df %>% mutate ( # https://dplyr.tidyverse.org/reference/mutate.html
    sentence.quartiles = case_when( # https://dplyr.tidyverse.org/reference/case_when.html 
      SENTENCE > 0.03 ~ 'above 0.03',
      SENTENCE > 15 ~ 'above first quartile',
      SENTENCE > 37  ~ 'above median',
      SENTENCE > 72  ~ 'above third quartile'
    ))

thresholds <- c(0.03, 15, 37, 72) 

table(df$sentence.quartiles) #quartiles are pretty evenly divided 

if(sum(is.na(df$sentence.quartiles)) != sum(df$SENTENCE <= 0.03)){
  print("uh oh, more NAs than unclassified values")
}

dim(df) #442187 x 28: 26 predictors, 1 continous response, 1 categorical response 
```

*All observations except for a few thousand have sentences of 0.03 months or more. df$sentence.quartiles are either "above 0.03" or NA.*


```{r}
###############
## functions ##
###############

odds.to.mean <- function(x){ x / (x + 1) }

mean.to.odds <- function(x){ x / (1 - x) }

## simulate posterior of fitted values for data matrix X
##   using samples from coefficient posterior

#beta.sims are logistic regression coefficents optimized through maximum likelihood estimation  
#I believe that this function is supposed to convert from the logit function, which is linear over the beta.sims, to the logistic function, whose output is the probability of receiving a sentence of a specified length 

#QUESTION: What's going on with this conversion from logit to logistic? Shouldn't it be exp(X %*% beta.sims) / (1 + exp(X %*% beta.sims))? 
logit.sims.predict <- function(X, beta.sims){
  as.matrix(1 / (1 + exp(-(X %*% beta.sims))))
} 

#QUESTION: Why the chunks? 
logit.sims.predict.chunked <- function(X, beta.sims, chunk.size = 100){
  if (ncol(beta.sims) %% chunk.size != 0){
    stop('# simulated coefficient draws is not evenly divisible by chunk size')
  }
  out <- llply( #applies the anonymous function to each element of the sequence
    seq(1, ncol(beta.sims), chunk.size),
    function(chunk.start){
      chunk.ind <- chunk.start + 0:(chunk.size - 1) #get indices of items in the chunk 
      #get predicted probabilities from model the simulated betas, transformed from logit 
      as.matrix(1 / (1 + exp(-(X %*% beta.sims[,chunk.ind])))) 
    })
  out <- do.call(cbind, out) 
  return(out) 
}

logit.sims.predictmean.chunked <- function(X, beta.sims, chunk.size = 100){
  if (ncol(beta.sims) %% chunk.size != 0){
    stop('# simulated coefficient draws is not evenly divisible by chunk size')
  }
  out <- llply(
    seq(1, ncol(beta.sims), chunk.size),
    function(chunk.start){
      chunk.ind <- chunk.start + 0:(chunk.size - 1)
      colMeans(as.matrix(1 / (1 + exp(-(X %*% beta.sims[,chunk.ind])))))
    })
  out <- unlist(out)
  return(out)
}

## adapted from sandwich::estfun
# https://www.rdocumentation.org/packages/sandwich/versions/3.0-1/topics/estfun 
estfun.speedglm <- function(mod, y, X){
  mu.eta <- mod$family$mu.eta 
  linkinv <- mod$family$linkinv #inverse link function of the glm
  variance <- mod$family$variance
  eta <- as.numeric(X %*% coefficients(mod)) 
  mu.eta.val <- mu.eta(eta)
  mu <- linkinv(eta)
  residuals <- (y - mu) / mu.eta.val
  weights <- mu.eta.val^2 / variance(mu)
  estfun <- residuals * weights * X #QUESTION: What's the meaning of this quantity? 
  return(estfun) 
}

## adapted from sandwich::estfun
#from sandwich documentation: "The meat of a clustered sandwich estimator is the cross product of the clusterwise summed estimating functions" (23)
#used for estimating the covariance matrix to get robust standard errors

meatCL.speedglm <- function(mod, cluster, y,  X, type = 'HC0'){
  if (is.list(mod) && !is.null(mod$na.action)) 
    class(mod$na.action) <- "omit"
  ef <- estfun.speedglm(mod, y, X)
  k <- NCOL(ef) #number of parameters in glm model 
  n <- NROW(ef) #number of observations
  rval <- matrix(0, nrow = k, ncol = k, dimnames = list(colnames(ef),
                                                        colnames(ef)))
  if (is.null(cluster))
    cluster <- attr(mod, "cluster")
  if (is.null(cluster))
    cluster <- 1L:n
  cluster <- as.data.frame(cluster)
  if (NROW(cluster) != n)
    stop("number of observations in 'cluster' and 'estfun()' do not match")
  p <- NCOL(cluster)
  if (p > 1L) { #if there are more than 1 columns 
    #get all possible subsets of  {1, 2, ..., p}. cl[,i] returns a vector containing the ith subset
    cl <- lapply(1L:p, function(i) combn(1L:p, i, simplify = FALSE)) 
    cl <- unlist(cl, recursive = FALSE)
    sign <- sapply(cl, function(i) (-1L)^(length(i) + 1L)) 
    for (i in (p + 1L):length(cl)) {
      cluster <- cbind(cluster, Reduce(paste0, cluster[, cl[[i]]]))
    }
  }
  else {
    cl <- list(1)
    sign <- 1
  }
  g <- sapply(1L:length(cl), function(i) { #sapply returns output of arg2(arg1) in a matrix 
    if (is.factor(cluster[[i]])) {
      length(levels(cluster[[i]]))
    }
    else {
      length(unique(cluster[[i]]))
    }
  })
  gmin <- min(g[1L:p])
  if (FALSE)
    g[] <- gmin
  type <- match.arg(type, c("HC", "HC0", "HC1"))
  if (type == "HC")
    type <- "HC0"
  for (i in 1L:length(cl)) {
    efi <- ef
    adj <- g[i]/(g[i] - 1L)
    efi <- if (g[i] < n)
             apply(efi, 2L, tapply, cluster[[i]], sum)
    else efi
    rval <- rval + sign[i] * adj * crossprod(efi)/n
  }
  if (type == "HC1")
    rval <- (n - 1L)/(n - k) * rval
  return(rval) 
}


## adapted from sandwich::estfun
vcovCL.speedglm <- function(mod, y, X, cluster = NULL,
                            sandwich = TRUE, fix = FALSE
                            ){
  rval <- meatCL.speedglm(mod, cluster = cluster, y, X)
  if (sandwich){
    bread. <- sandwich:::bread(mod)
    meat. <- rval
    n <- length(y)
    rval <- 1/n * (bread. %*% meat. %*% bread.)
  }
  if (fix && any((eig <- eigen(rval, symmetric = TRUE))$values <
                                                        0)) {
    eig$values <- pmax(eig$values, 0)
    rval <- crossprod(sqrt(eig$values) * t(eig$vectors))
  }
  return(rval)
}

compute.bounds.ci.from.samples <- function(min.stars, max.stars, alpha){
  sol = tryCatch(
  {
    # https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/optim 
    optim( #first arg = vector of params over which to optimize fn
      unname(quantile(max.stars, alpha + (1-alpha)/2)), #upper end of the CI with sig level alpha 
      fn = function (cihi){
        x = min.stars
        x[max.stars > cihi] = min(x) 
        cilo = quantile(x, 1 - alpha)
        return(cihi - cilo)
      },
      lower = quantile(max.stars, alpha),
      upper = quantile(max.stars, alpha + (1-alpha)/2),
      method = 'L-BFGS-B'
    )
  },
  error = function(e){
    NULL
  })
  if (!is.null(sol)){
    return(c(
      min.cilo = sol$par - sol$value, 
      max.cihi = sol$par
    ))
  } else {
    warning('caught error in HPD computation, returning conservative CI')
    return(c(
      min.cilo = quantile(min.stars, (1-alpha)/2, na.rm=TRUE),
      max.cihi = quantile(max.stars, alpha + (1-alpha)/2, na.rm=TRUE)
    ))
  }
}

```



```{r}
###########################
## create model matrices ##
###########################

### model formulas ###

formula.base <- formula(~ race)

formula.full <- formula(
  ~ race + AGE + MALE + HSGED + SOMEPOSTHS + POSTHSDEGREE + HISPANIC + USCITIZEN + CRIMINAL + CATEGORY2 + CATEGORY3 + CATEGORY4 + CATEGORY5 + CATEGORY6 + NOCOUNTS + POINTS + TRIAL + YR2001 + YR2002 + YR2003 + YR2004 + YR2005 + YR2006 + YR2007 + YR2008 + PRIM_OFFENSE
)

formula.full.race <- formula(
  ~ AGE + MALE + HSGED + SOMEPOSTHS + POSTHSDEGREE + HISPANIC + USCITIZEN + CRIMINAL + CATEGORY2 + CATEGORY3 + CATEGORY4 + CATEGORY5 + CATEGORY6 + NOCOUNTS + POINTS + TRIAL + YR2001 + YR2002 + YR2003 + YR2004 + YR2005 + YR2006 + YR2007 + YR2008 + PRIM_OFFENSE
)

### model matrices for running models ###
X.base <- sparse.model.matrix(formula.base, df) 
dim(X.base) #442187 rows, 2 cols. same nrows as df
#1st column is column of 1s, 2nd column is race data 
X.full <- sparse.model.matrix(formula.full, df)
#dim(X.full) #442187 x 57. There are 57 columns (as opposed to just 26) because each of 32 primary offense level gets its own column, minus one reference offense level 
X.full.race <- sparse.model.matrix(formula.full.race, df)

### model matrices for ate ###

## set all encounters to a particular race
d.setDwhite <- df; d.setDwhite$race <- factor('white', levels = races)
d.setDblack <- df; d.setDblack$race <- factor('black', levels = races)

## set race to white for all encounters
## note that in base specification, all encounters belong to the same
##   covariate strata so we can do interpretation with just one obs
# That is, the base specification has only one predictor (race), so all of the imputed treatment values are the same, and all the imputed control values are the same. This is why the model matrix only has 2 columns and 1 row.
X.base.setDwhite <- sparse.model.matrix(formula.base,
                                        d.setDwhite[1, , drop = FALSE],
                                        drop.unused.levels = FALSE
                                        )

X.full.setDwhite <- sparse.model.matrix(formula.full,
                                        d.setDwhite,
                                        drop.unused.levels = FALSE
                                        )

## set race to black for all encounters
X.base.setDblack <- sparse.model.matrix(formula.base,
                                        d.setDblack[1, , drop = FALSE],
                                        drop.unused.levels = FALSE
                                        )

X.full.setDblack <- sparse.model.matrix(formula.full,
                                        d.setDblack,
                                        drop.unused.levels = FALSE
                                        )

### model matrices for att ###

## avg effect of black (vs white) in black encounters
Xblack.base.setDwhite <- X.base.setDwhite
Xblack.base.setDblack <- X.base.setDblack
Xblack.full.setDwhite <- X.full.setDwhite[df$race == 'black',]
Xblack.full.setDblack <- X.full.setDblack[df$race == 'black',]

### cleanup ###
rm(d.setDwhite, d.setDblack)
```


### Base Model
```{r}
setwd("/Users/abbysteckel/Desktop/S&DS_491/KLM_sentencing_analysis/")
########################
## run correct models ##
########################

## predicted use of varying force against various racial groups, no covariates
if (!file.exists('mods_base_correct_blackwhite_sentencing_mislabeled.rds')){
  mods.base.correct <- lapply(
    1:length(thresholds),  # sentence thresholds. 1 = 'above first quartile." 2 = 'above median.' 3 = 'above third quartile.' 
    function(thresh_index){
      cat(rep('=', 80), '\nbase model, threshold = ', thresh_index,
          ' (', sentence.quartiles[thresh_index], ')\n\n', sep = '')
      mod <- speedglm.wfit(y = df$SENTENCE > thresholds[thresh_index], #indicator of whether force exceeded the given threshold
                           X = X.base,
                           family = binomial(link = 'logit'),
                           sparse = TRUE,
                           trace = TRUE
                           )
      vcovCL <- vcovCL.speedglm(mod,
                                df$SENTENCE > thresholds[thresh_index], 
                                X.base,
                                cluster = c(1:nrow(df)) #each observation is a cluster (ie no clustering)
                                )
      std_err <- sqrt(diag(vcovCL)) 
      return(list(thresh = sentence.quartiles[thresh_index], mod = mod, vcovCL = vcovCL, std_err = std_err))
    })
  saveRDS(mods.base.correct, 'mods_base_correct_blackwhite_sentencing_mislabeled.rds')
} else {
  mods.base.correct <- readRDS('mods_base_correct_blackwhite_sentencing_mislabeled.rds')
}

mods.base.correct 
#QUESTION: Shouldn't standard errors increase with decreasing sample size (ie higher threshold), in which case thresh = 0.03 would have the smallest standard errors?  

#interpretation of mods.base.correct
#QUESTION: is this the correct conversion from log odds to probability? 
log.odds.to.prob <- function(beta0, beta1, X){
  p <- exp(beta0 + beta1 %*% X)/(1 + exp(beta0 + beta1 %*% X))
}
#probability that a white person receives an above-first-quartile sentence 
(p.white.q3 <- log.odds.to.prob(mods.base.correct[[2]]$mod$coefficients[1], mods.base.correct[[2]]$mod$coefficients[2], 0))
#probability that a black person receives an above-first-quartile sentence 
(p.black.q3 <- log.odds.to.prob(mods.base.correct[[2]]$mod$coefficients[1], mods.base.correct[[2]]$mod$coefficients[2], 1))

#probability that a white person receives an above-median sentence 
(p.white.q2 <- log.odds.to.prob(mods.base.correct[[3]]$mod$coefficients[1], mods.base.correct[[3]]$mod$coefficients[2], 0))
#probability that a black person receives an above-median sentence 
(p.black.q2 <- log.odds.to.prob(mods.base.correct[[3]]$mod$coefficients[1], mods.base.correct[[3]]$mod$coefficients[2], 1))

#probability that a white person receives an above-third-quartile sentence 
(p.white.q1 <- log.odds.to.prob(mods.base.correct[[4]]$mod$coefficients[1], mods.base.correct[[4]]$mod$coefficients[2], 0))
#probability that a black person receives an above-third-quartile sentence 
(p.black.q1 <- log.odds.to.prob(mods.base.correct[[4]]$mod$coefficients[1], mods.base.correct[[4]]$mod$coefficients[2], 1))

```

### Full Model 
```{r}
## predicted use of varying force against various racial groups, with covariates
if (!file.exists('mods_full_correct_blackwhite_sentencing.rds')){
  mods.full.correct <- lapply(
    1:length(thresholds),  
    function(thresh_index){
      cat(rep('=', 80), '\nfull model, threshold = ', thresh_index,
          ' (', sentence.quartiles[thresh_index], ')\n\n', sep = '')
      mod <- speedglm.wfit(y = df$SENTENCE > thresholds[thresh_index],
                           X = X.full,
                           family = binomial(link = 'logit'),
                           sparse = TRUE,
                           trace = TRUE
                           )
      vcovCL <- vcovCL.speedglm(mod,
                                df$SENTENCE > thresholds[thresh_index],
                                X.full,
                                cluster = c(1:nrow(df))
                                )
      std_err <- sqrt(diag(vcovCL)) 
      return(list(thresh = sentence.quartiles[thresh_index], mod = mod, vcovCL = vcovCL, std_err = std_err))
    })
  saveRDS(mods.full.correct, 'mods_full_correct_blackwhite_sentencing.rds')
} else {
  mods.full.correct <- readRDS('mods_full_correct_blackwhite_sentencing.rds')
}

## predicted race (white vs not white) of civilian in encounter, with covariates
mod.race.full <- speedglm.wfit(y = df$race == 'white',
                               X = X.full.race,
                               family = binomial(link = 'logit'),
                               sparse = TRUE,
                               trace = TRUE
                               )

#here, I created a fake vector of covariate values by averaging the numeric and binary covariates of the third quartile observations. I arbitrarily looked at primary offense = assault. 
#QUESTION: Would it be more relevant to the ATE_M=1 estimator to average covariates over Black folks in the third quartile, instead of the entire third quartile? 
quant_covars <- c('AGE', 'MALE', 'HSGED', 'SOMEPOSTHS', 'POSTHSDEGREE', 'HISPANIC', 'USCITIZEN', 'CRIMINAL', 'CATEGORY2', 'CATEGORY3', 'CATEGORY4', 'CATEGORY5', 'CATEGORY6', 'NOCOUNTS', 'POINTS', 'TRIAL','YR2001', 'YR2002', 'YR2003', 'YR2004', 'YR2005', 'YR2006', 'YR2007', 'YR2008')

fake_x_q3_black.assault <- c(1, apply(df[df$sentence.quartiles %in% c("above third quartile"), quant_covars], 2, mean), 0, 0, 1, rep(0, 28))

#using the fake covariate vector of mean covariate values, I computed the probability of a black person receiving an above-third-quartile sentence for assault. p = 0.1576, which is surprisingly low.  
(p.black.full.q3.assault <- log.odds.to.prob(mods.full.correct[[1]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q3_black.assault))

fake_x_q3_white.assault <- c(0, apply(df[df$sentence.quartiles %in% c("above third quartile"), quant_covars], 2, mean), 0, 0, 1, rep(0, 28))
(p.white.full.q3.assault <- log.odds.to.prob(mods.full.correct[[1]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q3_white.assault)) #p = 0.096725 

fake_x_q2_black.assault <- c(1, apply(df[df$sentence.quartiles %in% c("above median"), quant_covars], 2, mean), 0, 0, 1, rep(0, 28))
#probability that a black person receives an above median sentence
(p.black.full.q2.assault <- log.odds.to.prob(mods.full.correct[[2]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q2_black.assault))

fake_x_q2_white.assault <- c(0, apply(df[df$sentence.quartiles %in% c("above median"), quant_covars], 2, mean), 0, 0, 1, rep(0, 28))
#probability that a white person receives an above median sentence
(p.white.full.q2.assault <- log.odds.to.prob(mods.full.correct[[2]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q2_white.assault)) 

fake_x_q1_black.assault <- c(1, apply(df[df$sentence.quartiles %in% c("above first quartile"), quant_covars], 2, mean), 0, 0, 1, rep(0, 28))
#probability that a black person receives an above median sentence
(p.black.full.q1.assault <- log.odds.to.prob(mods.full.correct[[3]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q1_black.assault))

fake_x_q1_white.assault <- c(0, apply(df[df$sentence.quartiles %in% c("above first quartile"), quant_covars], 2, mean), 0, 0, 1, rep(0, 28))
#probability that a white person receives an above median sentence
(p.white.full.q1.assault <- log.odds.to.prob(mods.full.correct[[3]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q1_white.assault)) 

#NOTE: the probability of being above the median is less than 0.5 for both races when the crime = assault. This is because assault earns relatively short sentences compared to other offenses (see murder below).

fake_x_q2_black.murder <-c(1, apply(df[df$sentence.quartiles %in% c("above median"), quant_covars], 2, mean), rep(0, 23), 1, rep(0, 7))
(p.black.full.q2.murder <- log.odds.to.prob(mods.full.correct[[2]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q2_black.murder))

fake_x_q2_white.murder <-c(0, apply(df[df$sentence.quartiles %in% c("above median"), quant_covars], 2, mean), rep(0, 23), 1, rep(0, 7))
(p.white.full.q2.murder <- log.odds.to.prob(mods.full.correct[[2]]$mod$coefficients[1], mods.full.correct[[1]]$mod$coefficients[2:57], fake_x_q2_white.murder)) #0.946
```

### Computing bounds 
```{r}
####################################
## bounds and ci for ates / atest ##
####################################

ndraws <- 5000
alpha <- .95
chunk.size <- 100
results.fname <- sprintf(
  'bounds_results_n%s_alpha%s_blackwhite_sentencing_mislabeled.rds',
  ndraws,
  alpha * 100
) 

rhos <- seq(.01, .99, .01) 
#rhos <- c(0.01, 0.99) tried using just two rhos when running the full model. still very slow. 

combos <- expand.grid(treat.race = 'black',
                      mod.type = c('base'), #NOTE: ultimately want to add 'full' 
                      thresh = 1:length(thresholds), #tried computing just thresh = 1, still very slow when using full models 
                      stringsAsFactors = FALSE
                      )

if (!file.exists(results.fname)){
  results <- ddply( # https://www.rdocumentation.org/packages/plyr/versions/1.8.6/topics/ddply 
    combos, #data frame to be processed 
    c('treat.race', 'mod.type', 'thresh'), #variables to split dataframe by 
    function(combo){

      ### prep ###

      treat.race <- combo$treat.race
      mod.type <- combo$mod.type
      thresh <- combo$thresh #thresh 1:3 corresponds to index of sentence.quartiles 
      cat(sprintf('starting %s %s %s\n', treat.race, mod.type, thresh))
      if (!mod.type %in% c('base', 'full')){
        stop('model specification not recognized')
      }
      if (!treat.race %in% c('black')){
        stop('treatment race not recognized')
      }

      ### intermediate values needed for qoi ### 
      #QUESTION: KLM call coef.stars "simulated coefficients." Why do we introduce randomness/simulation? Isn't the optimized (beta0, beta1) the same for all the data?

      ## simulated coefficients
      set.seed(02139)
      coef.stars <- t(mvrnorm( #mvrnorm() draws a random sample and fills a matrix by column
        n = ndraws, #n = 5000
        mu = coef(get(sprintf('mods.%s.correct', mod.type))[[thresh]]$mod), #coefficients beta0 and beta1 of the model with the given thresh 
        Sigma = get(sprintf('mods.%s.correct', mod.type))[[thresh]]$vcovCL #covariance matrix of the data, gives standard dev for normal draws 
      ))

      ### compute ates ###

      ## predicted probability of white civilian in encounter
      if (mod.type == 'full'){
          message("working on 'full'") #QUESTION: I know R hits this chunk of code because if I put stop("message"), it stops. How do I get a message to print? 
        D0.prob <-
          1 / (1 + exp(-(as.numeric(X.full.race %*% coef(mod.race.full))))) 
      } else {
        D0.prob <- mean(df$race == 'white')
      }

      ates.chunks <- llply(
        seq(1, ndraws, chunk.size),
        function(chunk.start){
          cat(sprintf('computing bounds in %s %s %s (draws %s:%s)\n',
                      treat.race,
                      mod.type,
                      thresh,
                      chunk.start,
                      chunk.start + chunk.size - 1
                      ))
          chunk.ind <- chunk.start + 0:(chunk.size - 1)
          ## for naive ate, set all encounters to a particular race, then
          ##   predict use of force in each encounter (row) by coef draw (col)
          pred.setD1.chunk <- logit.sims.predict(
            get(sprintf('X.%s.setD%s', mod.type, treat.race)), #X.base.setblack: data vectors where treatment is set to 1 
            coef.stars[,chunk.ind] #these are our betas 
          )
          pred.setD0.chunk <- logit.sims.predict(
            get(sprintf('X.%s.setDwhite', mod.type)),
            coef.stars[,chunk.ind]
          )
          ## for each encounter x coef draw, difference in probability of receiving sentence above the threshold 
          ##   when setting D=1 vs D=0
          # QUESTION: Is the above the correct interpretation of the ATE? KLM say it's the "difference in predicted force," which doesn't sound like a difference in probabilities 
          ates.naive.chunk <- colMeans(pred.setD1.chunk - pred.setD0.chunk)
          ## ates bounds and atest at various rhos (rows) and coef samples (cols)
          ## note: this duplicates vector D0.prob over columns of pred.setD0
          ates.bounds.lo.chunk <- laply(rhos, function(rho){
            ates.naive.chunk + colMeans(rho * pred.setD0.chunk * (1 - D0.prob))
          })
          ates.bounds.hi.chunk <- ates.bounds.lo.chunk +
            laply(rhos, function(rho){
              colMeans(
                rho / (1 - rho) * D0.prob * (
                  pred.setD1.chunk -
                    pmax(1 + pred.setD1.chunk / rho - 1 / rho, 0)
                )
              )
            })
          ## output
          return(list(ates.naive = ates.naive.chunk,
                      ates.bounds.lo = ates.bounds.lo.chunk,
                      ates.bounds.hi = ates.bounds.hi.chunk
                      )
                 )
        })

      ates.naive <- unlist(lapply(ates.chunks, function(x) x$ates.naive))
      ates.bounds.lo <- do.call(
        cbind,
        lapply(ates.chunks, function(x) x$ates.bounds.lo)
      )
      ates.bounds.hi <- do.call(
        cbind,
        lapply(ates.chunks, function(x) x$ates.bounds.hi)
      )
      rm(ates.chunks)

      ### compute atest ###

      ## for naive atest (minority encounters with real race, vs set to white)
      ## we never need the encounter-wise values for atest, so just store
      ## mean values (over all minority encounters) for each coef draw
      predD1.setD1 <- logit.sims.predictmean.chunked(
        get(sprintf('X%s.%s.setD%s', treat.race, mod.type, treat.race)),
        coef.stars,
        chunk.size = 100
      )
      predD1.setD0 <- logit.sims.predictmean.chunked(
        get(sprintf('X%s.%s.setDwhite', treat.race, mod.type)),
        coef.stars,
        chunk.size = 100
      )

      ## for each coef draw, difference in mean predicted force
      ##   when setting D=1 vs D=0
      atest.naive <- predD1.setD1 - predD1.setD0

      atest <- laply(rhos, function(rho){
        atest.naive + rho * predD1.setD0
      })

      ### output ###

      ## ates
      ates.naive.est <- mean(ates.naive)
      ates.naive.ci <- unname(quantile(
        ates.naive,
        c((1 - alpha) / 2, (alpha + 1) / 2)
      ))
      ates.bounds.est <- cbind(min = rowMeans(ates.bounds.lo),
                               max = rowMeans(ates.bounds.hi)
                               )
      ates.bounds.ci <- ldply(seq_along(rhos),
                              function(i){
                                compute.bounds.ci.from.samples(
                                  ates.bounds.lo[i,], #NOTE: the error message when I stopped the full bounds code from running said "Error in base::try(x, TRUE) : object 'x' not found." I think that means that this argument ates.bounds.lo[i,] wasn't created yet? 
                                  ates.bounds.hi[i,],
                                  alpha
                                )
                              })

      ## atest
      atest.naive.est <- mean(atest.naive)
      atest.naive.ci <- unname(quantile(
        atest.naive,
        c((1 - alpha) / 2, (alpha + 1) / 2)
      ))
      atest.est <- rowMeans(atest)
      atest.ci <- adply(atest,
                        1,
                        function(x){
                          out <- quantile(
                            x,
                            c((1 - alpha) / 2, (alpha + 1) / 2)
                          )
                          names(out) <- c('cilo', 'cihi')
                          return(out)
                        })

      cat(sprintf('finishing %s %s %s\n', treat.race, mod.type, thresh))

      return(rbind.fill(
        data.frame(qoi = 'ates',
                   rho = c(0, rhos),
                   est = c(ates.naive.est, rep(NA, length(rhos))),
                   est.cilo = c(ates.naive.ci[1], rep(NA, length(rhos))),
                   est.cihi = c(ates.naive.ci[2], rep(NA, length(rhos))),
                   min = c(NA, ates.bounds.est[, 'min']),
                   max = c(NA, ates.bounds.est[, 'max']),
                   min.cilo = c(NA, ates.bounds.ci[, 'min.cilo']), 
                   max.cihi = c(NA, ates.bounds.ci[, 'max.cihi'])
                   ),
        data.frame(qoi = 'atest',
                   rho = c(0, rhos),
                   est = c(atest.naive.est, atest.est),
                   est.cilo = c(atest.naive.ci[1], atest.ci[,'cilo']),
                   est.cihi = c(atest.naive.ci[2], atest.ci[,'cihi'])
                   )
      ))

    },
    .parallel = TRUE,
    .paropts =list(.export = c('mvrnorm', ls()[grep('^(X|mods)', ls())]))
  )
  results$rho <- round(results$rho, 2)  # fix minor floating point issue
  results$n.black <- sum(df$race == 'black')
  saveRDS(results, results.fname)
} else {
  results <- readRDS(results.fname)
}

#QUESTION: What's supposed to be the difference between min.cilo and min, and max.cihi and max? It looks like [min.cilo, max.cihi] tends to be wider than [min, max], consistent with KLM's plot.

#see cilo and cihi computation on line 265

dim(results) # 100 values of rho x 4 sentence thresholds x 2 estimands = 800 rows 
results[401:421, ] 

```

### Are these results consistent with expectations for rho = 0 and rho = 1? 
```{r}
mean(results[results$rho == 0, "est"]) #0.151  
mean(results[results$rho == 0.99, "est"], na.rm=TRUE) #0.717
#As expected, the estimated disparity in sentence length when we assume that there is significant racial bias in convictions is substantially (more than 4 times) larger than the naive disparity with no discrimination in convictions 
```
